#!/usr/bin/env python3
"""
Kompletna diagnostyka ≈õrodowiska serwera.
Uruchom to NA SERWERZE ≈ºeby znale≈∫ƒá problem.
"""

import sys
import os

print("=" * 70)
print("üîç DIAGNOSTYKA ≈öRODOWISKA SERWERA")
print("=" * 70)

# 1. Python version
print(f"\n1Ô∏è‚É£  Python Version:")
print(f"   {sys.version}")
print(f"   Executable: {sys.executable}")

# 2. CUDA environment
print(f"\n2Ô∏è‚É£  CUDA Environment Variables:")
cuda_vars = ['CUDA_VISIBLE_DEVICES', 'CUDA_HOME', 'LD_LIBRARY_PATH']
for var in cuda_vars:
    val = os.environ.get(var, 'NOT SET')
    print(f"   {var}: {val}")

# 3. PyTorch
print(f"\n3Ô∏è‚É£  PyTorch:")
try:
    import torch
    print(f"   ‚úì Version: {torch.__version__}")
    print(f"   ‚úì CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   ‚úì CUDA version: {torch.version.cuda}")
        print(f"   ‚úì GPU count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   ‚úì GPU {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"      Memory: {props.total_memory / 1024**3:.1f}GB")
    else:
        print(f"   ‚ùå CUDA NOT AVAILABLE!")
        print(f"   ‚ùå PyTorch zainstalowany BEZ wsparcia CUDA!")
        print(f"\n   FIX: Przeinstaluj PyTorch:")
        print(f"   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
except ImportError:
    print(f"   ‚ùå PyTorch nie zainstalowany!")

# 4. Transformers
print(f"\n4Ô∏è‚É£  Transformers:")
try:
    import transformers
    print(f"   ‚úì Version: {transformers.__version__}")
except ImportError:
    print(f"   ‚ùå Transformers nie zainstalowany!")

# 5. Accelerate (u≈ºywane przez device_map="auto")
print(f"\n5Ô∏è‚É£  Accelerate (kontroluje device_map='auto'):")
try:
    import accelerate
    print(f"   ‚úì Version: {accelerate.__version__}")
except ImportError:
    print(f"   ‚ö†Ô∏è  Accelerate nie zainstalowany!")
    print(f"   To mo≈ºe powodowaƒá problemy z device_map='auto'")

# 6. Bitsandbytes (dla 8-bit quantization)
print(f"\n6Ô∏è‚É£  Bitsandbytes (dla load_in_8bit):")
try:
    import bitsandbytes
    print(f"   ‚úì Version: {bitsandbytes.__version__}")
except ImportError:
    print(f"   ‚ö†Ô∏è  Bitsandbytes nie zainstalowany!")
    print(f"   8-bit quantization mo≈ºe nie dzia≈Çaƒá!")

# 7. Test GPU speed
print(f"\n7Ô∏è‚É£  Test szybko≈õci GPU:")
try:
    import torch
    import time
    
    if torch.cuda.is_available():
        # Small matmul test
        size = 2048
        device = "cuda:0"
        
        a = torch.randn(size, size, device=device, dtype=torch.float16)
        b = torch.randn(size, size, device=device, dtype=torch.float16)
        
        # Warmup
        for _ in range(3):
            _ = torch.matmul(a, b)
        torch.cuda.synchronize()
        
        # Measure
        start = time.time()
        for _ in range(10):
            c = torch.matmul(a, b)
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        ops_per_sec = (2 * size**3 * 10) / elapsed / 1e9  # GFLOPS
        
        print(f"   ‚úì Matrix multiply (2048x2048 FP16): {elapsed:.3f}s for 10 iterations")
        print(f"   ‚úì Performance: {ops_per_sec:.1f} GFLOPS")
        
        if ops_per_sec < 100:
            print(f"   ‚ö†Ô∏è  WOLNO! (powinno byƒá >1000 GFLOPS na RTX 5000 Ada)")
        elif ops_per_sec < 1000:
            print(f"   ‚ö†Ô∏è  Poni≈ºej oczekiwa≈Ñ dla RTX 5000 Ada")
        else:
            print(f"   ‚úì Szybko≈õƒá OK!")
    else:
        print(f"   ‚ùå Brak CUDA - nie mogƒô przetestowaƒá GPU")
except Exception as e:
    print(f"   ‚ùå B≈ÇƒÖd testu: {e}")

# 8. Test device_map="auto" behavior
print(f"\n8Ô∏è‚É£  Test device_map='auto':")
try:
    import torch
    from transformers import AutoModelForCausalLM
    
    if torch.cuda.is_available():
        print(f"   ≈Åadowanie ma≈Çego modelu (gpt2) z device_map='auto'...")
        
        model = AutoModelForCausalLM.from_pretrained(
            "gpt2",
            device_map="auto",
            torch_dtype=torch.float16,
        )
        
        # Check where it landed
        if hasattr(model, 'hf_device_map'):
            print(f"   ‚úì Device map: {model.hf_device_map}")
        
        device = next(model.parameters()).device
        print(f"   ‚úì Model device: {device}")
        
        if 'cpu' in str(device):
            print(f"   ‚ùå PROBLEM! Model trafi≈Ç na CPU mimo dostƒôpnego GPU!")
            print(f"   device_map='auto' NIE DZIA≈ÅA POPRAWNIE na tym serwerze!")
        else:
            print(f"   ‚úì Model poprawnie na GPU")
            
        del model
        torch.cuda.empty_cache()
    else:
        print(f"   ‚ùå Brak CUDA")
except Exception as e:
    print(f"   ‚ùå B≈ÇƒÖd: {e}")

# 9. Disk I/O speed (HuggingFace cache)
print(f"\n9Ô∏è‚É£  HuggingFace Cache:")
cache_dir = os.path.expanduser("~/.cache/huggingface")
print(f"   Path: {cache_dir}")
if os.path.exists(cache_dir):
    import subprocess
    try:
        # Check disk usage
        result = subprocess.run(['du', '-sh', cache_dir], capture_output=True, text=True)
        print(f"   Size: {result.stdout.strip()}")
        
        # Check if on slow filesystem
        result = subprocess.run(['df', '-h', cache_dir], capture_output=True, text=True)
        print(f"   Filesystem: {result.stdout.split()[0]}")
    except:
        pass
else:
    print(f"   ‚ö†Ô∏è  Cache nie istnieje")

print("\n" + "=" * 70)
print("üìã PODSUMOWANIE:")
print("=" * 70)

# Summary checks
issues = []

try:
    import torch
    if not torch.cuda.is_available():
        issues.append("‚ùå CRITICAL: PyTorch bez CUDA")
    elif torch.cuda.device_count() < 2:
        issues.append(f"‚ö†Ô∏è  WARNING: Wykryto {torch.cuda.device_count()} GPU (oczekiwano 2)")
except:
    issues.append("‚ùå CRITICAL: PyTorch nie zainstalowany")

try:
    import accelerate
except:
    issues.append("‚ö†Ô∏è  WARNING: Brak accelerate (device_map='auto' mo≈ºe nie dzia≈Çaƒá)")

try:
    import bitsandbytes
except:
    issues.append("‚ö†Ô∏è  WARNING: Brak bitsandbytes (load_in_8bit nie zadzia≈Ça)")

if issues:
    print("\nüî¥ Znalezione problemy:")
    for issue in issues:
        print(f"   {issue}")
else:
    print("\n‚úÖ ≈örodowisko wyglƒÖda OK!")
    print("   Problem mo≈ºe byƒá w innym miejscu (model, config, etc.)")

print("=" * 70)
