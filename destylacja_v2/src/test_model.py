#!/usr/bin/env python3
"""
Test wytrenowanego modelu studenta
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse

def load_model(model_path, base_model=None):
    """≈Åaduje wytrenowany model"""
    print(f"≈Åadowanie modelu z: {model_path}")
    
    # Je≈õli to model LoRA, trzeba za≈Çadowaƒá base + adapter
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            torch_dtype=torch.float16
        )
        print("‚úì Model za≈Çadowany (full model)")
    except:
        if base_model is None:
            base_model = "meta-llama/Llama-3.2-3B-Instruct"
        
        print(f"≈Åadowanie jako LoRA adapter (base: {base_model})")
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        base = AutoModelForCausalLM.from_pretrained(
            base_model,
            device_map="auto",
            torch_dtype=torch.float16
        )
        model = PeftModel.from_pretrained(base, model_path)
        print("‚úì Model za≈Çadowany (LoRA)")
    
    model.eval()
    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_tokens=512, temperature=0.7):
    """Generuje odpowied≈∫"""
    messages = [{"role": "user", "content": prompt}]
    
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.15,
            pad_token_id=tokenizer.eos_token_id
        )
    
    generated_ids = outputs[0][inputs.input_ids.shape[1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    return response

def interactive_mode(model, tokenizer):
    """Tryb interaktywny"""
    print("\n" + "="*60)
    print("TRYB INTERAKTYWNY")
    print("="*60)
    print("Wpisz 'quit' aby zako≈Ñczyƒá\n")
    
    while True:
        try:
            prompt = input("\nüéÆ Prompt: ")
            if prompt.lower() in ['quit', 'exit', 'q']:
                break
            
            if not prompt.strip():
                continue
            
            print("\nü§ñ Student odpowiada...")
            response = generate_response(model, tokenizer, prompt)
            print(f"\n{response}\n")
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\n\nZako≈Ñczono.")
            break
        except Exception as e:
            print(f"\nB≈ÇƒÖd: {e}")

def test_examples(model, tokenizer):
    """Testuje przyk≈Çadowe prompty"""
    examples = [
        "Hello how are you?",
        "What happened to you?",
        "What kind of city is this?",
        "Tell me about yourself",
        "What do you know about this place?"
    ]
    
    print("\n" + "="*60)
    print("TESTOWANIE PRZYK≈ÅADOWYCH PROMPT√ìW")
    print("="*60)
    
    for i, prompt in enumerate(examples, 1):
        print(f"\n{'='*60}")
        print(f"Przyk≈Çad {i}/{len(examples)}")
        print(f"{'='*60}")
        print(f"Prompt: {prompt}")
        print(f"\nOdpowied≈∫:")
        
        response = generate_response(model, tokenizer, prompt)
        print(response)
        print("\n")

def main():
    parser = argparse.ArgumentParser(description="Test wytrenowanego modelu")
    parser.add_argument("--model-path", default="../models/llama-3b-distilled",
                       help="≈öcie≈ºka do modelu")
    parser.add_argument("--base-model", default=None,
                       help="Base model (dla LoRA)")
    parser.add_argument("--interactive", action="store_true",
                       help="Tryb interaktywny")
    parser.add_argument("--prompt", type=str, default=None,
                       help="Pojedynczy prompt do przetestowania")
    
    args = parser.parse_args()
    
    # Za≈Çaduj model
    model, tokenizer = load_model(args.model_path, args.base_model)
    
    if args.prompt:
        # Pojedynczy prompt
        print(f"\nPrompt: {args.prompt}")
        print("\nOdpowied≈∫:")
        response = generate_response(model, tokenizer, args.prompt)
        print(response)
    elif args.interactive:
        # Tryb interaktywny
        interactive_mode(model, tokenizer)
    else:
        # Testuj przyk≈Çady
        test_examples(model, tokenizer)

if __name__ == "__main__":
    main()
