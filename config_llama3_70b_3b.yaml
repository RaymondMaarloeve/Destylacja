# Konfiguracja destylacji: Llama-3.2-11B → Llama-3.2-3B
# Nauczyciel: Llama-3.2-11B-Vision-Instruct (dobry do konwersacji)
# Student: Llama-3.2-3B-Instruct (mały, do deploymentu)

teacher_model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
  load_in_8bit: true
  max_new_tokens: 256
  temperature: 1.0  # ZMNIEJSZONE - zapobiega CUDA assert errors
  top_p: 0.85  # ZMNIEJSZONE dla stabilności

student_model:
  name: "Qwen/Qwen2-1.5B-Instruct"
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

data_generation:
  num_samples: 10000
  output_dir: "data"
  output_file: "teacher_dataset.jsonl"
  excel_file: "prompty.xlsx"

training:
  output_dir: "models/llama3_11b_to_3b_npc"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 2
  fp16: true
  gradient_checkpointing: true
  optim: "adamw_torch"
  max_grad_norm: 1.0
  kl_weight: 0.5  # Przeniesione z distillation
  ce_weight: 0.5  # Przeniesione z distillation

wandb:
  enabled: false  # Zmienione na false - wyłączamy wandb
  project: "llm-to-npc-llama3"
  run_name: "llama3.2-11b-to-3b"
