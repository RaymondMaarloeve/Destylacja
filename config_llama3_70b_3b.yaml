# Konfiguracja destylacji: Llama-3.2-11B → Llama-3.2-3B
# Nauczyciel: Llama-3.2-11B-Vision-Instruct (dobry do konwersacji)
# Student: Llama-3.2-3B-Instruct (mały, do deploymentu)

teacher_model:
  name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  load_in_8bit: true
  max_new_tokens: 256
  temperature: 2.0
  top_p: 0.95

student_model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

data_generation:
  num_samples: 10000
  output_path: "../../data/teacher_dataset.jsonl"

training:
  output_dir: "../../data/llama3_11b_to_3b_npc"
  num_epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 2
  fp16: true
  gradient_checkpointing: true
  optim: "adamw_torch"
  max_grad_norm: 1.0

distillation:
  kl_weight: 0.5
  ce_weight: 0.5
  temperature: 2.0

wandb:
  enabled: true
  project_name: "llm-to-npc-llama3"
  run_name: "llama3.2-11b-to-3b"
